# ğŸ™ï¸ Speaker-Aware Audio Transcription (Whisper + Pyannote)

This project performs **speech transcription** and **speaker diarization** using state-of-the-art AI models:

- **Faster-Whisper** â†’ High-accuracy speech-to-text
- **Pyannote** â†’ Speaker identification and segmentation

The output includes **timestamps**, **speaker labels**, and **transcribed dialogue** for every spoken chunk.

---

## ğŸš€ Features
âœ” Transcribes speech using Whisper (Large-V3)  
âœ” Identifies multiple speakers using Pyannote diarization  
âœ” Supports GPU (CUDA) acceleration  
âœ” Outputs speaker-tagged subtitles with time ranges  
âœ” Uses `.env` for secure Hugging Face token management  

---

## ğŸ“‚ Project Structure
.
â”œâ”€â”€ main.py
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

## 1. Create & Activate Virtual Environment

python -m venv venv
venv\Scripts\activate  # Windows
### or
source venv/bin/activate  # Linux/Mac

## 2. Install Dependencies

pip install -r requirements.txt

## 3. Add Hugging Face Token

Create a .env file in root:

HF_TOKEN=hf_your_token_here

## 4. Update Audio File Path in Code

Inside main.py, set your MP3/WAV file path:

AUDIO_FILE = r"Path\to\your\audio.mp3"

## 5. Run the Application

python main.py
